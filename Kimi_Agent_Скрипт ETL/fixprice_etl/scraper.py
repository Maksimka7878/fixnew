# ============================================
# Fix-Price ETL Pipeline - Web Scraper
# ============================================
"""
–ú–æ–¥—É–ª—å –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞–Ω–Ω—ã—Ö —Å fix-price.com –∏—Å–ø–æ–ª—å–∑—É—è Playwright –∏ BeautifulSoup.
"""

import asyncio
import re
from typing import List, Optional, Dict, Any, AsyncGenerator
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass

from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, Browser, BrowserContext
from fake_useragent import UserAgent
from loguru import logger

from models import Product, ProductSpecs, ProductImage, Category
from config import Config


@dataclass
class ScrapingConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞."""
    headless: bool = True
    browser_type: str = 'chromium'
    timeout: int = 30000
    navigation_timeout: int = 30000


class FixPriceScraper:
    """
    –°–∫—Ä–∞–ø–µ—Ä –¥–ª—è fix-price.com.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Playwright –¥–ª—è JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ –∏ BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML.
    """
    
    # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    SELECTORS = {
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
        'category_links': 'a[href*="/catalog/"]',
        'category_list': '.catalog-categories a, .category-item a, nav a[href*="/catalog/"]',
        
        # –¢–æ–≤–∞—Ä—ã –≤ —Å–ø–∏—Å–∫–µ
        'product_cards': '.product-card, .catalog-item, [data-product-id], .goods-item',
        'product_link': 'a[href*="/product/"], a.product-link',
        'product_title': '.product-title, .product-name, h1, .goods-title',
        'product_price': '.price-current, .product-price-current, [data-price], .price',
        'product_old_price': '.price-old, .product-price-old, .old-price',
        
        # –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–≤–∞—Ä–∞
        'product_page_title': 'h1, .product-detail h1, .product-info h1',
        'product_page_description': '.product-description, .description, [itemprop="description"]',
        'product_page_price': '.price-current, .product-price, [data-price]',
        'product_page_old_price': '.price-old, .old-price, .compare-price',
        'product_images': '.product-image img, .gallery-image img, .product-gallery img, [data-src]',
        'product_specs': '.product-specs, .specifications, .product-attributes',
        'spec_row': '.spec-row, .attribute-row, tr',
        'spec_name': '.spec-name, .attribute-name, td:first-child',
        'spec_value': '.spec-value, .attribute-value, td:last-child',
        'in_stock': '.in-stock, .available, [data-available="true"]',
        'out_of_stock': '.out-of-stock, .unavailable, [data-available="false"]',
        'sku': '.sku, .article, [data-sku]',
    }
    
    def __init__(self, config: Config, scraping_config: Optional[ScrapingConfig] = None):
        self.config = config
        self.scraping_config = scraping_config or ScrapingConfig(
            headless=config.HEADLESS,
            browser_type=config.BROWSER_TYPE
        )
        self.ua = UserAgent()
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±—Ä–∞—É–∑–µ—Ä–∞."""
        await self.init_browser()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞."""
        await self.close()
    
    async def init_browser(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Playwright –±—Ä–∞—É–∑–µ—Ä."""
        logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright –±—Ä–∞—É–∑–µ—Ä–∞...")
        
        self.playwright = await async_playwright().start()
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø –±—Ä–∞—É–∑–µ—Ä–∞
        if self.scraping_config.browser_type == 'firefox':
            browser_class = self.playwright.firefox
        elif self.scraping_config.browser_type == 'webkit':
            browser_class = self.playwright.webkit
        else:
            browser_class = self.playwright.chromium
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
        self.browser = await browser_class.launch(
            headless=self.scraping_config.headless,
            args=['--no-sandbox', '--disable-dev-shm-usage'] if self.scraping_config.headless else []
        )
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–∞–Ω–¥–æ–º–Ω—ã–º User-Agent
        self.context = await self.browser.new_context(
            user_agent=self.ua.random,
            viewport={'width': 1920, 'height': 1080},
            locale='ru-RU',
            timezone_id='Europe/Moscow'
        )
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã
        self.context.set_default_timeout(self.scraping_config.timeout)
        self.context.set_default_navigation_timeout(self.scraping_config.navigation_timeout)
        
        logger.info("‚úÖ –ë—Ä–∞—É–∑–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –±—Ä–∞—É–∑–µ—Ä."""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
        logger.info("üîí –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
    
    def _get_random_headers(self) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏."""
        return {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        }
    
    async def _create_page(self) -> Page:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ä–∞–Ω–¥–æ–º–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏."""
        page = await self.context.new_page()
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        await page.set_extra_http_headers(self._get_random_headers())
        
        # –ú–∞—Å–∫–∏—Ä—É–µ–º webdriver
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5]
            });
        """)
        
        return page
    
    async def get_page_content(self, url: str, wait_for_selector: Optional[str] = None) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç HTML-–∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Playwright.
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            wait_for_selector: –°–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
            
        Returns:
            HTML-–∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        page = await self._create_page()
        
        try:
            logger.debug(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞: {url}")
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = await page.goto(url, wait_until='networkidle')
            
            if not response or response.status >= 400:
                raise Exception(f"HTTP {response.status if response else 'Unknown'} –¥–ª—è {url}")
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if wait_for_selector:
                await page.wait_for_selector(wait_for_selector, timeout=10000)
            else:
                # –ñ–¥–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                await asyncio.sleep(1)  # –î–∞–µ–º –≤—Ä–µ–º—è –Ω–∞ JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥
            
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ lazy-–∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await self._scroll_page(page)
            
            content = await page.content()
            logger.debug(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(content)} bytes")
            
            return content
            
        finally:
            await page.close()
    
    async def _scroll_page(self, page: Page, scroll_delay: float = 0.5):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ lazy-–∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        try:
            # –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –≤–Ω–∏–∑ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ
            for _ in range(3):
                await page.evaluate('window.scrollBy(0, window.innerHeight)')
                await asyncio.sleep(scroll_delay)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –Ω–∞–≤–µ—Ä—Ö
            await page.evaluate('window.scrollTo(0, 0)')
            await asyncio.sleep(0.3)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–æ–ª–ª–µ: {e}")
    
    async def get_categories(self) -> List[Category]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        logger.info("üìÇ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        
        content = await self.get_page_content(
            self.config.FIX_PRICE_CATALOG_URL,
            wait_for_selector='.catalog-categories, .category-list, main'
        )
        
        soup = BeautifulSoup(content, 'lxml')
        categories = []
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for link in soup.select(self.SELECTORS['category_links']):
            href = link.get('href', '')
            name = link.get_text(strip=True)
            
            if href and name and '/catalog/' in href:
                url = urljoin(self.config.FIX_PRICE_BASE_URL, href)
                categories.append(Category(
                    name=name,
                    url=url,
                    level=href.count('/') - 1
                ))
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
        seen_urls = set()
        unique_categories = []
        for cat in categories:
            if cat.url not in seen_urls:
                seen_urls.add(cat.url)
                unique_categories.append(cat)
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(unique_categories)}")
        return unique_categories
    
    async def get_products_from_category(
        self, 
        category_url: str, 
        max_pages: Optional[int] = None
    ) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        
        Args:
            category_url: URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            max_pages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü (None = –≤—Å–µ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL —Ç–æ–≤–∞—Ä–æ–≤
        """
        logger.info(f"üìÑ –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
        
        product_urls = []
        page_num = 1
        
        while True:
            if max_pages and page_num > max_pages:
                break
            
            page_url = f"{category_url}?page={page_num}" if page_num > 1 else category_url
            
            try:
                content = await self.get_page_content(
                    page_url,
                    wait_for_selector='.product-card, .catalog-item, [data-product-id]'
                )
                
                soup = BeautifulSoup(content, 'lxml')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
                page_products = []
                for link in soup.select(self.SELECTORS['product_link']):
                    href = link.get('href', '')
                    if href and ('/product/' in href or '/goods/' in href):
                        full_url = urljoin(self.config.FIX_PRICE_BASE_URL, href)
                        page_products.append(full_url)
                
                if not page_products:
                    logger.debug(f"‚èπÔ∏è –ù–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
                    break
                
                product_urls.extend(page_products)
                logger.debug(f"   –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: {len(page_products)} —Ç–æ–≤–∞—Ä–æ–≤")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                pagination = soup.select('.pagination, .pager')
                next_page = soup.select_one('a[rel="next"], .next-page')
                
                if not next_page and len(page_products) < 12:  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 12 —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    break
                
                page_num += 1
                await asyncio.sleep(self.config.REQUEST_DELAY)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
                break
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        product_urls = list(dict.fromkeys(product_urls))
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(product_urls)}")
        
        return product_urls
    
    def _parse_price(self, price_text: Optional[str]) -> Optional[float]:
        """–ü–∞—Ä—Å–∏—Ç —Ü–µ–Ω—É –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
        if not price_text:
            return None
        
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –∏ –∑–∞–ø—è—Ç–æ–π/—Ç–æ—á–∫–∏
        cleaned = re.sub(r'[^\d.,]', '', price_text.replace(' ', '').replace('\xa0', ''))
        cleaned = cleaned.replace(',', '.')
        
        try:
            return float(cleaned) if cleaned else None
        except ValueError:
            return None
    
    def _extract_specs(self, soup: BeautifulSoup) -> ProductSpecs:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞."""
        specs = ProductSpecs()
        additional = {}
        
        specs_container = soup.select_one(self.SELECTORS['product_specs'])
        
        if specs_container:
            for row in specs_container.select(self.SELECTORS['spec_row']):
                name_elem = row.select_one(self.SELECTORS['spec_name'])
                value_elem = row.select_one(self.SELECTORS['spec_value'])
                
                if name_elem and value_elem:
                    name = name_elem.get_text(strip=True).lower()
                    value = value_elem.get_text(strip=True)
                    
                    if '–±—Ä–µ–Ω–¥' in name or 'brand' in name:
                        specs.brand = value
                    elif '–≤–µ—Å' in name or 'weight' in name:
                        specs.weight = value
                    elif '—Å—Ç—Ä–∞–Ω–∞' in name or 'country' in name:
                        specs.country = value
                    elif '—Ä–∞–∑–º–µ—Ä' in name or 'dimension' in name:
                        specs.dimensions = value
                    elif '–º–∞—Ç–µ—Ä–∏–∞–ª' in name or 'material' in name:
                        specs.material = value
                    else:
                        additional[name] = value
        
        specs.additional = additional
        return specs
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[ProductImage]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ç–æ–≤–∞—Ä–∞."""
        images = []
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ä–∞–∑–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        img_selectors = [
            '.product-image img',
            '.gallery-image img',
            '.product-gallery img',
            '[data-src]',
            '.swiper-slide img',
            '.product-photos img'
        ]
        
        found_urls = set()
        
        for selector in img_selectors:
            for img in soup.select(selector):
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                for attr in ['data-src', 'data-original', 'src', 'data-lazy']:
                    src = img.get(attr)
                    if src:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL
                        full_url = urljoin(base_url, src)
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º placeholder –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if 'placeholder' in full_url.lower() or 'data:image' in full_url:
                            continue
                        
                        # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–±–µ–∑ resize)
                        original_url = re.sub(r'/resize/\d+x\d+/', '/', full_url)
                        original_url = re.sub(r'\?w=\d+&h=\d+', '', original_url)
                        
                        if original_url not in found_urls:
                            found_urls.add(original_url)
                            images.append(ProductImage(
                                original_url=original_url,
                                is_primary=len(images) == 0
                            ))
                        break
        
        return images
    
    async def parse_product(self, product_url: str) -> Optional[Product]:
        """
        –ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–≤–∞—Ä–µ.
        
        Args:
            product_url: URL —Ç–æ–≤–∞—Ä–∞
            
        Returns:
            –û–±—ä–µ–∫—Ç Product –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        logger.debug(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–∞: {product_url}")
        
        try:
            content = await self.get_page_content(
                product_url,
                wait_for_selector='h1, .product-title'
            )
            
            soup = BeautifulSoup(content, 'lxml')
            
            # --- –ù–∞–∑–≤–∞–Ω–∏–µ ---
            title_elem = soup.select_one(self.SELECTORS['product_page_title'])
            if not title_elem:
                title_elem = soup.select_one('h1')
            title = title_elem.get_text(strip=True) if title_elem else None
            
            if not title:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: {product_url}")
                return None
            
            # --- –û–ø–∏—Å–∞–Ω–∏–µ ---
            description_elem = soup.select_one(self.SELECTORS['product_page_description'])
            description = description_elem.get_text(strip=True) if description_elem else None
            
            # --- –¶–µ–Ω—ã ---
            price_elem = soup.select_one(self.SELECTORS['product_page_price'])
            price = self._parse_price(price_elem.get_text(strip=True) if price_elem else None)
            
            old_price_elem = soup.select_one(self.SELECTORS['product_page_old_price'])
            old_price = self._parse_price(old_price_elem.get_text(strip=True) if old_price_elem else None)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ü–µ–Ω—É - —Ç–æ–≤–∞—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –æ—à–∏–±–∫–∞
            if price is None:
                logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Ü–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞: {product_url}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å price=0, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Ç–æ–≤–∞—Ä
                price = 0.0
            
            # --- –ù–∞–ª–∏—á–∏–µ ---
            in_stock = True
            if soup.select_one(self.SELECTORS['out_of_stock']):
                in_stock = False
            elif '–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏' in content.lower():
                in_stock = False
            
            # --- SKU ---
            sku_elem = soup.select_one(self.SELECTORS['sku'])
            sku = sku_elem.get_text(strip=True) if sku_elem else None
            
            # --- –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ ---
            specs = self._extract_specs(soup)
            
            # --- –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ---
            images = self._extract_images(soup, product_url)
            
            # --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ ---
            categories_path = []
            breadcrumbs = soup.select('.breadcrumb a, .breadcrumbs a, [itemprop="itemListElement"] a')
            for crumb in breadcrumbs:
                cat_name = crumb.get_text(strip=True)
                if cat_name and cat_name.lower() not in ['–≥–ª–∞–≤–Ω–∞—è', 'home']:
                    categories_path.append(cat_name)
            
            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç —Ç–æ–≤–∞—Ä–∞
            product = Product(
                source_id=sku or self._extract_product_id(product_url),
                source_url=product_url,
                title=title,
                description=description,
                price=price,
                old_price=old_price,
                category=categories_path[-1] if categories_path else None,
                categories_path=categories_path,
                specs=specs,
                images=images,
                in_stock=in_stock,
                sku=sku,
                processed=True
            )
            
            logger.debug(f"‚úÖ –¢–æ–≤–∞—Ä —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {title[:50]}... | –¶–µ–Ω–∞: {price}")
            return product
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞ {product_url}: {e}")
            return None
    
    def _extract_product_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ ID –≤ URL
        match = re.search(r'/product[s]?/(\d+)', url)
        if match:
            return match.group(1)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à URL
        import hashlib
        return hashlib.md5(url.encode()).hexdigest()[:12]
    
    async def parse_products_batch(
        self, 
        product_urls: List[str],
        progress_callback=None
    ) -> List[Product]:
        """
        –ü–∞—Ä—Å–∏—Ç –±–∞—Ç—á —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º concurrency.
        
        Args:
            product_urls: –°–ø–∏—Å–æ–∫ URL —Ç–æ–≤–∞—Ä–æ–≤
            progress_callback: Callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
        """
        products = []
        semaphore = asyncio.Semaphore(self.config.CONCURRENCY_LIMIT)
        
        async def parse_with_limit(url: str) -> Optional[Product]:
            async with semaphore:
                product = await self.parse_product(url)
                if progress_callback:
                    progress_callback()
                await asyncio.sleep(self.config.REQUEST_DELAY)
                return product
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
        tasks = [parse_with_limit(url) for url in product_urls]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º concurrency
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Product):
                products.append(result)
            elif isinstance(result, Exception):
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ: {result}")
        
        return products
